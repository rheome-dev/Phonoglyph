# Story 5.4: Audio Feature Extraction & Mapping

**Epic**: 5 - Stem Separation & Audio Analysis  
**Story**: 5.4  
**Status**: Complete ✅  
**Priority**: High  
**Estimated Effort**: 16 hours  
**Dependencies**: Story 5.2 ✅  
**Progress**: 100% Complete

## User Story

**As a** music producer or content creator  
**I want to** have the system extract meaningful musical features from my stems and cache them  
**So that** my visualizations respond naturally to the musical structure without manual configuration or real-time processing overhead

## Technical Implementation Details

### Comprehensive Feature Extraction System
```typescript
interface ExtractedAudioFeatures {
  // Basic Audio Properties
  frequencies: Float32Array;     // FFT frequency data
  timeData: Float32Array;        // Time domain waveform
  volume: number;                // Overall RMS level
  bass: number;                  // Low frequency energy (0-60 Hz)
  mid: number;                   // Mid frequency energy (60-2000 Hz)
  treble: number;                // High frequency energy (2000-20000 Hz)
  
  // Advanced Spectral Features
  rms: number;                   // Root Mean Square energy
  spectralCentroid: number;      // Brightness/frequency center
  spectralRolloff: number;       // Frequency distribution cutoff
  spectralFlatness: number;      // Tone vs. noise ratio
  spectralSpread: number;        // Frequency concentration
  spectralFlux: number;          // Spectral change over time
  
  // Perceptual Features
  loudness: number;              // Perceived loudness
  perceptualSpread: number;      // Perceived spectral spread
  perceptualSharpness: number;   // Perceived sharpness
  
  // Timbral Features
  mfcc: number[];                // Mel-frequency cepstral coefficients
  totalEnergy: number;           // Total signal energy
  
  // Musical Structure Features
  beats: Array<{                 // Beat detection results
    time: number;
    confidence: number;
  }>;
  onsets: Array<{                // Note onset detection
    time: number;
    strength: number;
  }>;
  peaks: Array<{                 // Energy peaks
    time: number;
    value: number;
  }>;
  drops: Array<{                 // Energy drops
    time: number;
    intensity: number;
  }>;
}

interface StemFeatureAnalysis {
  stemType: 'original' | 'drums' | 'bass' | 'vocals' | 'other';
  features: ExtractedAudioFeatures;
  waveformData: {
    points: number[];
    sampleRate: number;
    duration: number;
  };
  analysisMetadata: {
    version: string;
    duration: number;           // Analysis processing time
    bufferSize: number;
    hopSize: number;
  };
}

class AudioFeatureExtractor {
  private meyda: MeydaAnalyzer;
  private featureExtractors: string[];

  constructor() {
    this.featureExtractors = [
      'rms',
      'zcr',
      'spectralCentroid',
      'spectralRolloff',
      'spectralFlatness',
      'spectralSpread',
      'spectralFlux',
      'mfcc',
      'loudness',
      'perceptualSpread',
      'perceptualSharpness'
    ];
  }

  async extractFeatures(audioBuffer: Buffer, stemType: string): Promise<StemFeatureAnalysis> {
    // Initialize Meyda with optimal settings for caching
    // Process full audio buffer in chunks
    // Extract all specified features
    // Generate beat detection and onset analysis
    // Create waveform visualization data
    // Return comprehensive analysis
  }
}
```

### Feature Mapping System
```typescript
interface FeatureMapping {
  feature: keyof ExtractedAudioFeatures;
  visualProperty: 'scale' | 'rotation' | 'color' | 'emission' | 'position';
  transform: {
    range: [number, number];  // Output range
    scale: 'linear' | 'log' | 'exponential';
    smoothing: number;  // 0-1 smoothing factor
  };
  stemType?: 'original' | 'drums' | 'bass' | 'vocals' | 'other';
}

interface VisualizationMapping {
  id: string;
  name: string;
  description: string;
  mappings: FeatureMapping[];
  stemWeights: {
    original: number;
    drums: number;
    bass: number;
    vocals: number;
    other: number;
  };
}

class FeatureMapper {
  private mappings: Map<string, VisualizationMapping>;

  constructor() {
    this.mappings = new Map();
    this.initializeDefaultMappings();
  }

  private initializeDefaultMappings() {
    // Default mappings for common visualization patterns
    this.mappings.set('rhythm-driven', {
      id: 'rhythm-driven',
      name: 'Rhythm-Driven Visualization',
      description: 'Visualization driven by beat detection and rhythmic patterns',
      mappings: [
        {
          feature: 'beats',
          visualProperty: 'scale',
          transform: { range: [0.5, 2.0], scale: 'exponential', smoothing: 0.3 }
        },
        {
          feature: 'rms',
          visualProperty: 'emission',
          transform: { range: [0, 1], scale: 'linear', smoothing: 0.2 }
        }
      ],
      stemWeights: { original: 0.3, drums: 0.5, bass: 0.2, vocals: 0.0, other: 0.0 }
    });

    this.mappings.set('melody-driven', {
      id: 'melody-driven',
      name: 'Melody-Driven Visualization',
      description: 'Visualization responding to melodic and harmonic content',
      mappings: [
        {
          feature: 'spectralCentroid',
          visualProperty: 'color',
          transform: { range: [0, 360], scale: 'linear', smoothing: 0.4 }
        },
        {
          feature: 'mfcc',
          visualProperty: 'rotation',
          transform: { range: [0, 360], scale: 'linear', smoothing: 0.3 }
        }
      ],
      stemWeights: { original: 0.4, drums: 0.1, bass: 0.2, vocals: 0.3, other: 0.0 }
    });
  }

  mapFeaturesToVisualization(
    stemAnalyses: StemFeatureAnalysis[],
    mappingId: string
  ): VisualizationParameters {
    const mapping = this.mappings.get(mappingId);
    if (!mapping) {
      throw new Error(`Unknown mapping: ${mappingId}`);
    }

    // Combine features from all stems according to weights
    const combinedFeatures = this.combineStemFeatures(stemAnalyses, mapping.stemWeights);
    
    // Apply mappings to generate visualization parameters
    return this.applyMappings(combinedFeatures, mapping.mappings);
  }

  private combineStemFeatures(
    analyses: StemFeatureAnalysis[],
    weights: Record<string, number>
  ): ExtractedAudioFeatures {
    // Weighted combination of features from different stems
    // Implementation details for feature blending
  }

  private applyMappings(
    features: ExtractedAudioFeatures,
    mappings: FeatureMapping[]
  ): VisualizationParameters {
    // Apply each mapping to generate visualization parameters
    // Implementation details for parameter generation
  }
}
```

### Caching Strategy

**Cache Key Structure:** `(file_metadata_id, stem_type, analysis_version)`
- **File Isolation:** Each file has unique analysis
- **Stem Granularity:** Original + each separated stem tracked separately  
- **Version Control:** Analysis version enables cache invalidation
- **User Security:** RLS ensures users only access their own data

**Performance Optimizations:**
- Guest users (ID starting with 'guest_') bypass caching
- Indexed database queries for fast retrieval
- Memory-efficient streaming instead of full buffer loading
- Background processing via queue workers

## Acceptance Criteria

### ✅ Feature Extraction
- [x] **Comprehensive Analysis**: 15+ audio features extracted per stem
- [x] **Beat Detection**: Accurate beat detection with confidence scores
- [x] **Onset Analysis**: Note onset detection for musical structure
- [x] **Spectral Features**: Full spectral analysis including MFCC
- [x] **Perceptual Features**: Loudness and perceptual characteristics

### ✅ Feature Mapping
- [x] **Mapping System**: Flexible mapping of features to visual properties
- [x] **Stem Weighting**: Configurable weights for different stem types
- [x] **Transformation Functions**: Linear, logarithmic, and exponential scaling
- [x] **Smoothing**: Configurable smoothing for stable visualizations
- [x] **Preset Mappings**: Pre-configured mappings for common patterns

### ✅ Caching & Performance
- [x] **Database Caching**: Efficient storage and retrieval of analysis
- [x] **Background Processing**: Non-blocking analysis during upload
- [x] **Memory Optimization**: Streaming processing for large files
- [x] **Fast Retrieval**: Sub-100ms access to cached analysis
- [x] **Scalability**: Background queue processing for multiple users

### ✅ Integration
- [x] **Stem Integration**: Analysis during stem separation process
- [x] **API Endpoints**: Clean API for feature retrieval
- [x] **Error Handling**: Graceful handling of analysis failures
- [x] **Version Control**: Analysis versioning for improvements
- [x] **User Isolation**: RLS policies for data security

## Technical Dependencies

### External Libraries
- Meyda.js for comprehensive audio feature extraction
- Node.js audio processing libraries
- Queue processing system

### Internal Dependencies
- Story 5.2: Audio analysis integration & caching ✅
- Database schema and RLS policies
- R2 storage for file access

## Success Metrics

- [x] Analysis completed during upload/stem separation process
- [x] Cached retrieval in <100ms for typical queries
- [x] Memory usage optimized via streaming
- [x] Analysis covers 15+ audio features for rich visualization control
- [x] Background processing scales with user load

## Utility Infrastructure

**Queue Worker:** Background processing for analysis jobs  
**Backfill Script:** Analyzes existing files without cached analysis  
**Test Scripts:** Validation of caching system functionality  

## Dev Agent Record

- **Status**: Complete ✅ (100% Complete)
- **Key Achievement**: Comprehensive feature extraction with 15+ audio features
- **Integration**: Seamless integration with stem separation and caching
- **Performance**: Optimized for visualization engine requirements
