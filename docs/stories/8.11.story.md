# Story 8.11: Audio Processing Pipeline Monitoring and Alerting

**Epic**: 8 - Code Quality & Production Stability  
**Story**: 8.11  
**Status**: Not Started ðŸ”´  
**Priority**: High  
**Estimated Effort**: 8 hours  
**Dependencies**: Story 8.10 (Monitoring stack)

## User Story

**As a** operations team  
**I want** comprehensive monitoring and alerting for the audio processing pipeline  
**So that** I can detect and resolve audio processing issues before they impact users

## Technical Implementation Details

### **Current Audio Pipeline Monitoring Gaps**
The audio processing pipeline currently lacks:
- Real-time health monitoring for stem separation
- Audio analysis performance tracking
- Queue depth and processing time alerts
- Failed processing job detection
- Resource utilization monitoring

### **Audio Pipeline Monitoring Strategy**

#### **Phase 1: Pipeline Health Checks (3 hours)**
```typescript
// apps/api/src/services/audio-pipeline-monitor.ts
export class AudioPipelineMonitor {
  private healthChecks = new Map<string, HealthCheck>();
  
  async checkStemSeparationHealth(): Promise<HealthStatus> {
    try {
      // Check Trigger.dev job queue
      const queueDepth = await this.getQueueDepth('stem-separation');
      const avgProcessingTime = await this.getAverageProcessingTime('stem-separation', '1h');
      const failureRate = await this.getFailureRate('stem-separation', '1h');
      
      const status: HealthStatus = {
        service: 'stem-separation',
        healthy: queueDepth < 100 && avgProcessingTime < 300000 && failureRate < 0.05,
        metrics: {
          queueDepth,
          avgProcessingTime,
          failureRate,
        },
        timestamp: new Date(),
      };
      
      if (!status.healthy) {
        await this.triggerAlert('stem-separation-degraded', status);
      }
      
      return status;
    } catch (error) {
      await this.triggerAlert('stem-separation-error', { error: error.message });
      throw error;
    }
  }
  
  async checkAudioAnalysisHealth(): Promise<HealthStatus> {
    const recentAnalyses = await this.db
      .select()
      .from(audioEventsCache)
      .where(
        gte(audioEventsCache.createdAt, new Date(Date.now() - 3600000)) // Last hour
      );
    
    const successRate = recentAnalyses.length > 0 ? 
      recentAnalyses.filter(a => a.timeline !== null).length / recentAnalyses.length : 1;
    
    return {
      service: 'audio-analysis',
      healthy: successRate > 0.95,
      metrics: {
        totalAnalyses: recentAnalyses.length,
        successRate,
        avgProcessingTime: this.calculateAvgProcessingTime(recentAnalyses),
      },
      timestamp: new Date(),
    };
  }
  
  async checkStorageHealth(): Promise<HealthStatus> {
    try {
      // Test R2 storage connectivity
      const testKey = `health-check-${Date.now()}`;
      await this.r2Client.putObject({
        Bucket: process.env.R2_BUCKET_NAME!,
        Key: testKey,
        Body: 'health-check',
      });
      
      await this.r2Client.deleteObject({
        Bucket: process.env.R2_BUCKET_NAME!,
        Key: testKey,
      });
      
      return {
        service: 'storage',
        healthy: true,
        metrics: { connectivity: 'ok' },
        timestamp: new Date(),
      };
    } catch (error) {
      return {
        service: 'storage',
        healthy: false,
        metrics: { error: error.message },
        timestamp: new Date(),
      };
    }
  }
}
```

#### **Phase 2: Real-time Alerting System (3 hours)**
```typescript
// apps/api/src/services/alerting.ts
export class AlertingService {
  private alertChannels: AlertChannel[] = [];
  
  constructor() {
    this.setupAlertChannels();
  }
  
  private setupAlertChannels() {
    // Sentry for critical errors
    this.alertChannels.push({
      name: 'sentry',
      severity: ['critical', 'high'],
      handler: this.sendSentryAlert.bind(this),
    });
    
    // Slack for operational alerts
    this.alertChannels.push({
      name: 'slack',
      severity: ['critical', 'high', 'medium'],
      handler: this.sendSlackAlert.bind(this),
    });
    
    // Email for business-critical issues
    this.alertChannels.push({
      name: 'email',
      severity: ['critical'],
      handler: this.sendEmailAlert.bind(this),
    });
  }
  
  async triggerAlert(alertType: AlertType, data: AlertData) {
    const alert: Alert = {
      id: crypto.randomUUID(),
      type: alertType,
      severity: this.getAlertSeverity(alertType),
      message: this.formatAlertMessage(alertType, data),
      data,
      timestamp: new Date(),
    };
    
    // Store alert in database
    await this.db.insert(alerts).values(alert);
    
    // Send to appropriate channels
    const channels = this.alertChannels.filter(
      channel => channel.severity.includes(alert.severity)
    );
    
    await Promise.all(
      channels.map(channel => channel.handler(alert))
    );
    
    // Track alert in PostHog
    serverPostHog.capture({
      distinctId: 'system',
      event: 'alert_triggered',
      properties: {
        alertType,
        severity: alert.severity,
        service: data.service,
      },
    });
  }
  
  private getAlertSeverity(alertType: AlertType): AlertSeverity {
    const severityMap: Record<AlertType, AlertSeverity> = {
      'stem-separation-degraded': 'high',
      'stem-separation-error': 'critical',
      'audio-analysis-degraded': 'medium',
      'storage-error': 'critical',
      'queue-overflow': 'high',
      'processing-timeout': 'medium',
      'high-failure-rate': 'high',
    };
    
    return severityMap[alertType] || 'medium';
  }
  
  private async sendSlackAlert(alert: Alert) {
    const webhook = process.env.SLACK_WEBHOOK_URL;
    if (!webhook) return;
    
    const payload = {
      text: `ðŸš¨ ${alert.severity.toUpperCase()}: ${alert.message}`,
      attachments: [{
        color: this.getSlackColor(alert.severity),
        fields: [
          { title: 'Service', value: alert.data.service, short: true },
          { title: 'Time', value: alert.timestamp.toISOString(), short: true },
          { title: 'Alert ID', value: alert.id, short: true },
        ],
      }],
    };
    
    await fetch(webhook, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(payload),
    });
  }
}
```

#### **Phase 3: Performance Metrics Dashboard (2 hours)**
```typescript
// apps/api/src/routers/monitoring.ts
export const monitoringRouter = router({
  getAudioPipelineHealth: protectedProcedure
    .query(async ({ ctx }) => {
      const monitor = new AudioPipelineMonitor(ctx.db);
      
      const [stemHealth, analysisHealth, storageHealth] = await Promise.all([
        monitor.checkStemSeparationHealth(),
        monitor.checkAudioAnalysisHealth(),
        monitor.checkStorageHealth(),
      ]);
      
      return {
        overall: stemHealth.healthy && analysisHealth.healthy && storageHealth.healthy,
        services: {
          stemSeparation: stemHealth,
          audioAnalysis: analysisHealth,
          storage: storageHealth,
        },
        timestamp: new Date(),
      };
    }),
  
  getProcessingMetrics: protectedProcedure
    .input(z.object({
      timeRange: z.enum(['1h', '24h', '7d', '30d']).default('24h'),
      service: z.enum(['stem-separation', 'audio-analysis', 'all']).default('all'),
    }))
    .query(async ({ ctx, input }) => {
      const timeRangeMs = {
        '1h': 3600000,
        '24h': 86400000,
        '7d': 604800000,
        '30d': 2592000000,
      }[input.timeRange];
      
      const startTime = new Date(Date.now() - timeRangeMs);
      
      // Get processing metrics from usage tracking
      const metrics = await ctx.db
        .select()
        .from(featureUsage)
        .where(
          and(
            gte(featureUsage.timestamp, startTime),
            input.service !== 'all' ? 
              eq(featureUsage.feature, input.service) : 
              inArray(featureUsage.feature, ['stem_separation', 'ai_analysis'])
          )
        )
        .orderBy(featureUsage.timestamp);
      
      return {
        totalOperations: metrics.length,
        successRate: this.calculateSuccessRate(metrics),
        avgProcessingTime: this.calculateAvgProcessingTime(metrics),
        throughput: metrics.length / (timeRangeMs / 3600000), // operations per hour
        timeline: this.groupMetricsByTime(metrics, input.timeRange),
      };
    }),
  
  getActiveAlerts: protectedProcedure
    .query(async ({ ctx }) => {
      const activeAlerts = await ctx.db
        .select()
        .from(alerts)
        .where(
          and(
            eq(alerts.resolved, false),
            gte(alerts.timestamp, new Date(Date.now() - 86400000)) // Last 24 hours
          )
        )
        .orderBy(desc(alerts.timestamp));
      
      return activeAlerts;
    }),
});
```

### **File Locations**
**Monitoring Services**:
- `apps/api/src/services/audio-pipeline-monitor.ts` - Pipeline health monitoring
- `apps/api/src/services/alerting.ts` - Alert management system
- `apps/api/src/services/metrics-collector.ts` - Performance metrics collection

**API Layer**:
- `apps/api/src/routers/monitoring.ts` - Monitoring API endpoints
- `apps/api/src/middleware/performance-tracking.ts` - Request performance tracking

**Database Schema**:
- `apps/api/src/db/schema/monitoring.ts` - Alerts and health check tables
- `apps/api/src/db/migrations/021_monitoring.sql` - Monitoring tables migration

**Frontend Dashboard**:
- `apps/web/src/pages/admin/monitoring.tsx` - Monitoring dashboard
- `apps/web/src/components/monitoring/` - Monitoring UI components

## Acceptance Criteria

### **Health Monitoring Requirements**
- [ ] **Real-time health checks for all audio processing services**
- [ ] **Queue depth monitoring with configurable thresholds**
- [ ] **Processing time tracking and trend analysis**
- [ ] **Failure rate monitoring with automatic alerting**
- [ ] **Storage connectivity and performance monitoring**

### **Alerting Requirements**
- [ ] **Multi-channel alerting (Slack, email, Sentry)**
- [ ] **Configurable alert thresholds and severity levels**
- [ ] **Alert deduplication and rate limiting**
- [ ] **Automatic alert resolution tracking**
- [ ] **Alert escalation for unresolved critical issues**

### **Performance Monitoring**
- [ ] **Processing time percentiles (p50, p95, p99)**
- [ ] **Throughput monitoring and capacity planning**
- [ ] **Resource utilization tracking**
- [ ] **Historical trend analysis**
- [ ] **Performance regression detection**

### **Dashboard Requirements**
- [ ] **Real-time pipeline health overview**
- [ ] **Historical performance metrics**
- [ ] **Active alerts and incident tracking**
- [ ] **Service dependency visualization**
- [ ] **Capacity planning insights**

## Success Metrics

### **Operational Metrics**
- [ ] **Mean time to detection <2 minutes**
- [ ] **Mean time to resolution <15 minutes**
- [ ] **Alert accuracy >95% (low false positives)**
- [ ] **Pipeline uptime >99.5%**

### **Performance Metrics**
- [ ] **Audio processing success rate >98%**
- [ ] **Average processing time within SLA**
- [ ] **Queue overflow incidents <1 per month**
- [ ] **Storage availability >99.9%**

### **Business Impact Metrics**
- [ ] **Reduced customer support tickets related to processing**
- [ ] **Improved user satisfaction with processing reliability**
- [ ] **Proactive issue resolution before user impact**
- [ ] **Data-driven capacity planning and optimization**